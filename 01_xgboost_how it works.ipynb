{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Теория XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Проблема многих алгоритмов построения деревьев в том, что в них не уделяется должного внимания регуляризации. В классическом градиентном бустинге применяются такие меры:\n",
    "\n",
    " - ограничение на структуру дерева: максимальная глубина (max_depth), минимальное число объектов в листе (min_samples_leaf)\n",
    " - контролирование темпа обучения (learning_rate)\n",
    " - увеличение \"непохожести\" деревьев за счет рандомизации, как в случайном лесе"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Модель**\n",
    "\n",
    "Пусть $y_i$ – значение переменной, которое необходимо предсказать, $x_i$ – входные данные.\n",
    "\n",
    "Модель имеет вид\n",
    "$$\\hat{y}_i = \\sum_{k=1}^K f_k(x_i),\\hspace{10pt} f_k \\in \\mathcal{F},$$\n",
    "где $K$ – количество деревьев, $f$ – функция на пространстве $\\mathcal{F}$, которое содержит все возможные деревья решений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost использует еще больше параметров для регуляризации базовых деревьев."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Целевая функция**\n",
    "$$\\text{Obj}(\\theta) = L(\\theta) + \\Omega(\\theta),$$\n",
    "где \n",
    "* $\\theta$ – параметры модели;\n",
    "* $L$ – величина потерь на обучающей выборке  (насколько хорошо модель описывает данные);\n",
    "* $\\Omega$ – компонента, отвечающая за регуляризацию (насколько модель сложная)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция потерь зависит от решаемой задачи (Xgboost адаптирован под задачи классификации, регрессии и ранжирования, а регуляризатор выглядит следующим образом:\n",
    "\n",
    "$$\n",
    "\\Omega(f) = \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^{T}w_j^2\n",
    "$$\n",
    "\n",
    "Первое слагаемое ($\\gamma T$) штрафует модель за большое число листьев $T$, а второе ($\\frac{1}{2} \\lambda \\sum_{j=1}^{T}w_j^2$) контролирует сумму весов модели в листьях. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Обучение**\n",
    "\n",
    "Необходимо обучить функции $f_i$, каждая из которых включает структуру дерева и значения листьев.\n",
    "\n",
    "Обозначим $\\hat{y}_i^{(t)}$ предсказанное значение на шаге $t$. Тогда целевая функция имеет вид:\n",
    "$$\\text{Obj} = \\sum_{i=1}^n l\\left(y_i, \\hat{y}_i^{(t)}\\right) + \\sum_{i=1}^t\\Omega(f_i).$$\n",
    "\n",
    "Обучение деревьев происходит поочередно, начиная с постоянного предсказания:\n",
    "$$\\begin{split}\\hat{y}_i^{(0)} &= 0\\\\\n",
    "\\hat{y}_i^{(1)} &= f_1(x_i) = \\hat{y}_i^{(0)} + f_1(x_i)\\\\\n",
    "\\hat{y}_i^{(2)} &= f_1(x_i) + f_2(x_i)= \\hat{y}_i^{(1)} + f_2(x_i)\\\\\n",
    "&\\dots\\\\\n",
    "\\hat{y}_i^{(t)} &= \\sum_{k=1}^t f_k(x_i)= \\hat{y}_i^{(t-1)} + f_t(x_i)\\end{split}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На каждом шаге выбирается дерево, которое оптимизирует целевую функцию.\n",
    "$$\\begin{split}\\text{Obj}^{(t)} & = \\sum_{i=1}^n l\\left(y_i, \\hat{y}_i^{(t)}\\right) + \\sum_{i=1}^t\\Omega(f_i) \\\\\n",
    "          & = \\sum_{i=1}^n l\\left(y_i, \\hat{y}_i^{(t-1)} + f_t(x_i)\\right) + \\Omega(f_t) + \\mathrm{constant}\\end{split}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для упрощения задачи оптимизации для заданной функции потерь используется разложение Тейлора:\n",
    "$$F(x+\\Delta x) \\simeq F(x) + F'(x)\\Delta x + \\frac{1}{2} F''(x)\\Delta x^2 + \\dots$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тогда обозначив градиент и гессиан функции потерь соответственно\n",
    "$$g_i = \\partial_{\\hat{y}_i^{(t-1)}} l(y_i, \\hat{y}_i^{(t-1)}),\\hspace{10pt}h_i = \\partial_{\\hat{y}_i^{(t-1)}}^2 l(y_i, \\hat{y}_i^{(t-1)}),$$\n",
    "целевая функция будет иметь вид\n",
    "$$\\text{Obj}^{(t)} = \\sum_{i=1}^n [l(y_i, \\hat{y}_i^{(t-1)}) + g_i f_t(x_i) + \\frac{1}{2} h_i f_t^2(x_i)] + \\Omega(f_t) + \\mathrm{constant}.$$\n",
    "\n",
    "Убирая константы на шаге $t$, целевая функция упрощается в виде:\n",
    "$$\\text{Obj}^{(t)} = \\sum_{i=1}^n [g_i f_t(x_i) + \\frac{1}{2} h_i f_t^2(x_i)] + \\Omega(f_t).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, величина потерь $L$ зависит только от $g_i$ и $h_i$.\n",
    "\n",
    "Благодаря этому, XGBoost поддерживает пользовательские целевые функции, для которых достаточно задать градиент и гессиан функции потерь."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Регуляризация**\n",
    "\n",
    "Для начала определим дерево $f_t(x)$:\n",
    "$$f_t(x) = w_{q(x)}, w \\in R^T, q:R^d\\rightarrow \\{1,2,\\cdots,T\\},$$\n",
    "где $w$ – вектор значений на листьях дерева, $T$ – количество листьев, $q$ – функция, которая каждой точке набора данных ставит в соответствие лист дерева.\n",
    "\n",
    "Тогда сложность модели имеет вид\n",
    "$$\\Omega(f) = \\gamma T + \\frac{1}{2}\\lambda \\sum_{j=1}^T w_j^2+ \\alpha \\sum_{j=1}^T |w_j|,$$\n",
    "где $\\gamma$ – штраф на сложность деревьев, $\\lambda$ – сила регуляризации $\\ell_2$, $\\alpha$ – сила регуляризации $\\ell_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Переобучение**\n",
    "\n",
    "Для контроля переобучения помимо параметров $\\gamma$, $\\alpha$ и $\\lambda$ используются также параметры:\n",
    "* Контролирующие сложность модели напрямую \n",
    "    * максимальная глубина дерева (`max_depth`)\n",
    "    * минимальный вес в узле, ниже которого прекращается дальнейшее разделение в этом узле (`min_child_weight`)\n",
    "* Добавляющие случайность, повышая устойчивость к зашумлению\n",
    "    * $\\eta$ – величина шага. $\\eta \\in (0,1]$.\n",
    "    Вместо $\\hat{y}_i^{(t)} = \\hat{y}_i^{(t-1)} + f_t(x_i)$ используется \n",
    "    $\\hat{y}_i^{(t)} = \\hat{y}_i^{(t-1)} + \\eta\\cdot f_t(x_i)$\n",
    "    * Доля подвыборки наблюдений для построения деревьев (`subsample`)\n",
    "    * Доля подвыборки признаков для построения деревьев (`colsample_bytree`)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
